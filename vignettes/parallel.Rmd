---
title: "Running tests in parallel"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Running tests in parallel}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
editor_options:
  markdown:
    wrap: sentence
---

```{r setup, include = FALSE}
library(testthat)
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

To take advantage of parallel tests, add the following line to the `DESCRIPTION`:

    Config/testthat/parallel: true

## Basic operation

Starting a new R process is relatively expensive, so testthat begins by creating a pool of workers.
The size of the pool will be determined by `getOptions("Ncpus")`, then the `TESTTHAT_MAX_CPUS` envvar, then the number of processes reported by `ps::ps_cpu_count()`.
In any case, testthat will never start more subprocesses than test files.

Each worker begins by loading testthat and the package being tested.
It then runs any setup files (so if you have existing setup files you'll need to make sure they work when executed in parallel).

testthat then starts sending test files to workers in alphabetical order: as soon as a subprocess has finished, it receives another file, until all files are done.
This means that state is persisted across test files: options are *not* reset, loaded packages are *not* unloaded, the global environment is *not* cleared, etc.
You are responsible for making sure each file leaves the world as it finds it.

Because files are run in alphabetical order, you may want to rename your slowest test files so that they start first, e.g. `test-1-slowest.R`, `test-2-next-slowest.R`, etc.

## Common problems

-   If tests fail stochastically (i.e. they sometimes work and sometimes fail) you may have accidentally introduced a dependency between your test files.
    This sort of dependency is hard to track down due to the random nature, and you'll need to check all tests to make sure that they're not accidentally changing global state.

-   If you use [packaged scope test fixtures](https://testthat.r-lib.org/articles/test-fixtures.html#package), you'll need to review them to make sure that they work in parallel.
    For example, if you were previously creating a temporary database in the test directory, you'd need to instead create it in the session temporary directory so that each process gets its on independent version.

## Performance

There is some overhead associated with running tests in parallel:

-   Startup cost is linear in the number of subprocesses, because we need to create them in a loop.
    This is about 50ms on my laptop.
    Each subprocess needs to load testthat and the tested package, this happens in parallel, and we cannot do too much about it.

-   Clean up time is again linear in the number of subprocesses, and it about 80ms per subprocess on my laptop.

-   It seems that sending a message (i.e. a passing or failing expectation) is about 2ms currently.
    This is the total cost that includes sending the message, receiving it, and replying it to a non-parallel reporter.

This overhead generally means that if you have many test files that take a short amount of time, you're unlikely to see a huge benefit by using parallel tests.
For example, testthat itself takes about 10s to run tests in serial, and 8s to run the tests in parallel.
That's because most tests take less than a second to run, and the slowest test (which takes 3s) is started last.
